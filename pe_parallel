#ifndef PE_PARALLEL_
#define PE_PARALLEL_

#include "pe_base"

#if PLATFORM_WIN

#include <windows.h>
#include <process.h>

// The MultiThreadsTaskRunner focuses how to create thread pool,
// send request to thread.
template <typename Task>
class MultiThreadsTaskRunner {
 public:
  struct TaskItem {
    Task task;
    int64 task_item_id;
    int worker_idx;
    int is_stop_task;
  };

 private:
  enum {
    min_thread = 1,
    max_thread = 64,
    max_pending_tasks = 1024,  // 1<<20,  // the actual maximum pending task is
                               // max_pending_tasks - 1
  };

#if PE_HAS_CPP11
  using worker_ptr_t = std::function<void(TaskItem)>*;
#else
  typedef void (*worker_ptr_t)(TaskItem task_item);
#endif

  struct ThreadContext {
    int idx;
    worker_ptr_t worker_ptr;
    MultiThreadsTaskRunner* host;
  };

 public:
  MultiThreadsTaskRunner() : worker_ptr_(nullptr) {
    memset(threads_context_, 0, sizeof threads_context_);
  }

  void Reset() {
    threads_count_ = 0;
    worker_ptr_ = nullptr;
    p_ = q_ = 0;
    memset(threads_context_, 0, sizeof threads_context_);
  }

  void Init(worker_ptr_t worker, const int threads = 3,
            const int stack = 100 * (1 << 20)) {
    assert(worker_ptr_ == nullptr);

    p_ = q_ = 0;
    InitializeCriticalSection(&request_access_);
    has_new_request_ = CreateEvent(nullptr, FALSE, FALSE, nullptr);
    request_removed_ = CreateEvent(nullptr, FALSE, FALSE, nullptr);
    threads_count_ = max(threads, static_cast<int>(min_thread));
    threads_count_ = min(threads_count_, static_cast<int>(max_thread));
    worker_ptr_ = worker;
    for (int i = 0; i < threads_count_; ++i) {
      threads_context_[i].idx = i;
      threads_context_[i].worker_ptr = worker_ptr_;
      threads_context_[i].host = this;
      threads_handle_[i] = reinterpret_cast<HANDLE>(_beginthreadex(
          nullptr, max(stack, 100 * (1 << 20)), WorkThread,
          reinterpret_cast<void*>(&threads_context_[i]), 0, nullptr));
    }
    start_ = GetTickCount();
  }

  void WaitForQueue(int max_request = -1) {
    if (max_request == -1) max_request = threads_count_;
    if (max_request > max_pending_tasks - 1) {
      max_request = max_pending_tasks - 1;
    }
    for (;;) {
      if (RequestCount() < max_request) break;
      WaitForSingleObject(request_removed_, static_cast<DWORD>(INFINITY));
    }
  }

  void AddRequest(const Task& task, int is_stop_task = 0) {
    EnterCriticalSection(&request_access_);
    PE_ASSERT((q_ + 1) % static_cast<int64>(max_pending_tasks) != p_);
    TaskItem item = {task, next_task_item_id_++, -1, is_stop_task};
    task_que_[q_++] = item;
    if (q_ >= max_pending_tasks) {
      q_ -= max_pending_tasks;
    }
    LeaveCriticalSection(&request_access_);
    SetEvent(has_new_request_);
  }

  void WaitForEnd() {
    WaitForQueue();
    AddRequest(Task(), 1);
    WaitForMultipleObjects(threads_count_, threads_handle_, TRUE, INFINITE);
    for (int i = 0; i < threads_count_; ++i) CloseHandle(threads_handle_[i]);
    CloseHandle(request_removed_);
    CloseHandle(has_new_request_);
    DeleteCriticalSection(&request_access_);
    end_ = GetTickCount();
    fprintf(stderr, "time : %u\n", static_cast<unsigned>(end_ - start_));
  }

  void Lock() { EnterCriticalSection(&request_access_); }

  void Unlock() { LeaveCriticalSection(&request_access_); }

 private:
  // inc = 0 or 1
  bool NextRequest(TaskItem& item, int inc) {
    item.task_item_id = -1;
    int oldp = p_;
    EnterCriticalSection(&request_access_);
    if (p_ != q_) {
      item = task_que_[p_];
      if (inc > 0 && !item.is_stop_task) {
        p_ += inc;
        if (p_ >= max_pending_tasks) {
          p_ -= max_pending_tasks;
        }
      }
    }
    LeaveCriticalSection(&request_access_);
    if (oldp != p_) {
      SetEvent(request_removed_);
    }
    return item.task_item_id != -1;
  }

  int RequestCount() {
    int ret = 0;
    EnterCriticalSection(&request_access_);
    ret = q_ - p_;
    if (ret < 0) {
      ret += max_pending_tasks;
    }
    LeaveCriticalSection(&request_access_);
    return ret;
  }

  void Work(ThreadContext thread_context) {
    for (;;) {
      TaskItem item;
      if (NextRequest(item, 0) == false) {
        DWORD wait_result = WaitForSingleObject(has_new_request_, INFINITE);
        if (wait_result != WAIT_OBJECT_0) continue;
      }
      if (NextRequest(item, 1) == false) continue;
      if (item.is_stop_task) {
        SetEvent(has_new_request_);
        break;
      }
      if (thread_context.worker_ptr) {
        item.worker_idx = thread_context.idx;
        (*thread_context.worker_ptr)(item);
      }
    }
  }

  static unsigned int __stdcall WorkThread(void* p) {
    ThreadContext context_copy = *reinterpret_cast<ThreadContext*>(p);
    context_copy.host->Work(context_copy);
    return 0;
  }

 private:
  DWORD start_, end_;

 private:
  int threads_count_{0};

  HANDLE threads_handle_[max_thread];
  ThreadContext threads_context_[max_thread];

  HANDLE has_new_request_;
  HANDLE request_removed_;
  CRITICAL_SECTION request_access_;
  worker_ptr_t worker_ptr_;

  TaskItem task_que_[max_pending_tasks + 1];
  int64 next_task_item_id_{0};
  int p_{0}, q_{0};
};

enum {
  PRIORITY_REALTIME = REALTIME_PRIORITY_CLASS,
  PRIORITY_HIGH = HIGH_PRIORITY_CLASS,
  PRIORITY_ABOVE_NORMAL = ABOVE_NORMAL_PRIORITY_CLASS,
  PRIORITY_NORMAL = NORMAL_PRIORITY_CLASS,
  PRIORITY_BELOW_NORMAL = BELOW_NORMAL_PRIORITY_CLASS,
  PRIORITY_BACKGROUND = 0x00100000,  // PROCESS_MODE_BACKGROUND_BEGIN,
  PRIORITY_IDLE = IDLE_PRIORITY_CLASS,
};

static inline void SetProcessPriority(int priority) {
  ::SetPriorityClass(::GetCurrentProcess(), priority);
}

static inline void MakeSureProcessSingleton(const char* id) {
  string mutex_name = "pe_mutex_prefix_";
  mutex_name += id;
  HANDLE hMutex = ::OpenMutex(MUTEX_ALL_ACCESS, FALSE, mutex_name.c_str());
  if (hMutex) {
    fprintf(stderr, "another process is running\n");
    ::CloseHandle(hMutex);
    exit(-1);
    return;
  }
  hMutex = ::CreateMutex(nullptr, TRUE, mutex_name.c_str());
  if (::GetLastError() == ERROR_ALREADY_EXISTS) {
    fprintf(stderr, "another process is running\n");
    ::CloseHandle(hMutex);
    exit(-1);
    return;
  }
}

#if PE_HAS_CPP11

#include <iostream>
#include "pe_time"
#include "pe_persistance"

template <typename Derived>
struct RangeBasedTaskGenerator {
  struct Task {
    int64 id;
    int operator==(const Task& other) const { return id == other.id; }
  };

  RangeBasedTaskGenerator(int64 first = 0, int64 last = 0, int64 block_size = 1)
      : first_(first), last_(last), block_size_(block_size) {
    assert(block_size_ >= 1);
    const int64 cnt = last_ - first_ + 1;
    first_task_ = 1;
    last_task_ = (cnt + block_size - 1) / block_size;
  }

  ~RangeBasedTaskGenerator() = default;

  Derived& SetRange(int64 first = 0, int64 last = 0, int64 block_size = 1) {
    first_ = first;
    last_ = last;
    block_size_ = block_size;
    assert(block_size_ >= 1);
    const int64 cnt = last_ - first_ + 1;
    first_task_ = 1;
    last_task_ = (cnt + block_size - 1) / block_size;
    return static_cast<Derived&>(*this);
  }

  Task FirstTask() {
    Task first = {first_task_};
    return first;
  }

  Task NextTask(Task curr) {
    curr.id++;
    return curr;
  }

  Task LastTask() {
    Task last = {last_task_};
    return last;
  }

  pair<int64, int64> TaskIdToRange(int64 id) const {
    assert(id >= first_task_ && id <= last_task_);

    int64 u = (id - 1) * block_size_ + first_;
    int64 v = u + block_size_ - 1;
    if (v > last_) v = last_;

    return {u, v};
  }
  int64 block_size_;
  int64 first_;
  int64 last_;
  int64 first_task_;
  int64 last_task_;
};

template <typename Derived>
struct SupportCacheFileName {
  SupportCacheFileName(const char* /*file_name*/ = nullptr) {}

  const char* CacheFileName() const { return file_name_; }

  Derived& SetCacheFileName(const char* file_name) {
    file_name_ = file_name;
    return static_cast<Derived&>(*this);
  }

  const char* file_name_{nullptr};
};

template <typename CONTEXT, bool has_file_name>
struct GetCacheFileNameHelper {
  const char* Get(CONTEXT& context) { return context.CacheFileName(); }
};

template <typename CONTEXT>
struct GetCacheFileNameHelper<CONTEXT, false> {
  const char* Get(CONTEXT& /*context*/) { return nullptr; }
};

// The implementation of task scheduler.
// It uses an instance of MultiThreadsTaskRunner to implement task scheduler,
// and provides the process of tasking handling, it allows to add code in
// different process stage (based on callback in context).
// MultiThreadsTaskRunner focuses on how to create thread and send request to
// a thread.
template <typename CONTEXT>
class ParallelRunner {
  using runner_t = MultiThreadsTaskRunner<typename CONTEXT::Task>;
  using TaskItem = typename runner_t::TaskItem;

 public:
  ParallelRunner() = default;

  ~ParallelRunner() = default;

  void WorkOnThread(TaskItem ti) {
    // step 1: check cache
    Lock();
    bool can_skip = OnStart(ti);
    Unlock();
    if (can_skip) return;

    // step 2: run
    TimeRecorder tr;
    const int64 local_result = context_ptr_->Work(ti.task, ti.worker_idx);
    auto usage = tr.Elapsed();

    // step 3: finish
    Lock();
    OnStop(ti, local_result, usage);
    Unlock();
  }

  bool OnStart(const TaskItem& task_item) {
    cerr << task_item.task.id << " begins" << endl;
    if (kv_) {
      auto iter = kv_->storage().find(task_item.task.id);
      if (iter != kv_->storage().end()) {
        const int64 old_value = iter->second;
        return context_ptr_->HandleCachedResult(task_item.task, old_value);
      }
    }
    return false;
  }

  bool OnStop(const TaskItem& task_item, int64 local_result, TimeDelta usage) {
    cerr << task_item.task.id << " finishes. (" << usage.Format() << ")"
         << endl;
    cerr << "local_result = " << local_result << endl;
    if (kv_) kv_->Set(task_item.task.id, local_result);
    context_ptr_->HandleResult(task_item.task, local_result);
    cerr << "result = " << context_ptr_->Result() << endl;
    return true;
  }

  void Lock() { oml_.Lock(); }

  void Unlock() { oml_.Unlock(); }

  int64 Run(CONTEXT&& context, int threads_count) {
    return Run(context, threads_count);
  }

  int64 Run(CONTEXT& context, int threads_count) {
    const char* file_name =
        GetCacheFileNameHelper<
            CONTEXT,
            is_base_of<SupportCacheFileName<CONTEXT>, CONTEXT>::value>()
            .Get(context);
    if (file_name && file_name[0]) kv_ = new KVPersistance(file_name);
    context_ptr_ = &context;

    std::function<void(TaskItem)> worker(
        [=](TaskItem ti) { WorkOnThread(ti); });

    if (threads_count > 0) {
      oml_.Init(&worker, threads_count);
    } else {
      oml_.Init(&worker);
    }
    auto curr = context.FirstTask();
    auto last = context.LastTask();
    for (;;) {
      oml_.WaitForQueue();
      oml_.AddRequest(curr);
      if (curr == last) break;
      curr = context.NextTask(curr);
    }
    oml_.WaitForEnd();
    oml_.Reset();
    context.OnFinished();

    if (kv_) {
      delete kv_;
      kv_ = nullptr;
    }
    context_ptr_ = nullptr;

    return context.Result();
  }

 private:
  runner_t oml_;
  KVPersistance* kv_{nullptr};
  CONTEXT* context_ptr_;
};

// The parallel interface that used to support macro syntax.
// It uses ParallelRunner to schedule taskes.
// It inherits RangeBasedTaskGenerator, SupportCacheFileName to
// generate tasks and support cache.
class ParallelRange : public RangeBasedTaskGenerator<ParallelRange>,
                      public SupportCacheFileName<ParallelRange> {
 public:
  ParallelRange(int64 first = 0, int64 last = 0, int64 block_size = 1,
                const char* file_name = nullptr)
      : RangeBasedTaskGenerator(first, last, block_size),
        SupportCacheFileName(file_name) {}

  int64 Result() const { return result_; }

  void OnFinished() { cerr << "result = " << result_ << endl; }

  void HandleResult(const Task& /*task*/, int64 result) {
    if (reducer_internal_) {
      result_ = (this->*reducer_internal_)(result_, result);
    } else {
      result_ = update_result_(result_, result);
    }
  }

  bool HandleCachedResult(const Task& task, int64 result) {
    HandleResult(task, result);
    return true;
  }

  int64 Work(const Task& task, int64 worker_idx) {
    auto range = TaskIdToRange(task.id);
    if (has_wob_) {
      return work_on_block_(range.first, range.second, worker_idx);
    }
    assert(has_woi_);

    int64 t = work_on_item_(range.first, worker_idx);
    if (reducer_internal_) {
      for (int64 i = range.first + 1; i <= range.second; ++i) {
        t = (this->*reducer_internal_)(t, work_on_item_(i, worker_idx));
      }
    } else {
      for (int64 i = range.first + 1; i <= range.second; ++i) {
        t = update_result_(t, work_on_item_(i, worker_idx));
      }
    }
    return t;
  }

  ParallelRange& From(int64 first) {
    first_ = first;
    return *this;
  }

  ParallelRange& To(int64 last) {
    last_ = last;
    return *this;
  }

  ParallelRange& DividedBy(int64 block_size) {
    block_size_ = block_size;
    return *this;
  }

  ParallelRange& SetCacheFileName(const char* file_name) {
    SupportCacheFileName<ParallelRange>::SetCacheFileName(file_name);
    return *this;
  }

  ParallelRange& SetResult(int64 result) {
    result_ = result;
    return *this;
  }

  ParallelRange& SetThreadsCount(int threads_count) {
    threads_count_ = threads_count;
    return *this;
  }

  ParallelRange& Map(std::function<int64(int64, int64)> woi) {
    work_on_item_ = std::move(woi);
    has_woi_ = true;
    return *this;
  }

  ParallelRange& BlockMap(std::function<int64(int64, int64, int64)> wor) {
    work_on_block_ = std::move(wor);
    has_wob_ = true;
    return *this;
  }

  ParallelRange& Reduce(std::function<int64(int64, int64)> ur) {
    update_result_ = std::move(ur);
    return *this;
  }

  ParallelRange& SetReduceSum(bool reduce_sum) {
    reduce_sum_ = reduce_sum;
    return *this;
  }

  ParallelRange& SetReduceProd(bool reduce_prod) {
    reduce_prod_ = reduce_prod;
    return *this;
  }

  ParallelRange& SetMod(int64 mod) {
    mod_ = mod;
    return *this;
  }

  ParallelRange& Start() {
    SetRange(first_, last_, block_size_);
    BindReducer();
    runner_.Run(*this, threads_count_);
    return *this;
  }

  void Run(int threads_count) { runner_.Run(*this, threads_count); }

 protected:
  void BindReducer() {
    if (reduce_sum_) {
      if (mod_) {
        reducer_internal_ = &ParallelRange::ReduceAddMod;
      } else {
        reducer_internal_ = &ParallelRange::ReduceAdd;
      }
      reducer_init_value_ = 0;
    } else if (reduce_prod_) {
      if (mod_) {
        reducer_internal_ = &ParallelRange::ReduceProdMod;
      } else {
        reducer_internal_ = &ParallelRange::ReduceProd;
      }
      reducer_init_value_ = 1;
    }
    if (reducer_internal_) {
      result_ = reducer_init_value_;
    }
  }

  int64 ReduceAdd(int64 result, int64 value) { return result + value; }

  int64 ReduceAddMod(int64 result, int64 value) {
    return AddMod(result, RegulateMod(value, mod_), mod_);
  }

  int64 ReduceProd(int64 result, int64 value) { return result * value; }

  int64 ReduceProdMod(int64 result, int64 value) {
    return MulModEx(result, RegulateMod(value, mod_), mod_);
  }

 protected:
  ParallelRunner<ParallelRange> runner_;
  int64 result_{0};
  int threads_count_{8};
  bool has_woi_{false};
  bool has_wob_{false};
  bool reduce_sum_{false};
  bool reduce_prod_{false};
  int64 mod_{0};
  int64 reducer_init_value_{0};
  using ReducerType = int64 (ParallelRange::*)(int64, int64);
  ReducerType reducer_internal_{nullptr};
  std::function<int64(int64, int64)> update_result_;
  std::function<int64(int64, int64)> work_on_item_;
  std::function<int64(int64, int64, int64)> work_on_block_;
};

#define PARALLEL_RANGE ParallelRange()
#define BEGIN_PARALLEL ParallelRange()
#define FROM                  .From((
#define TO                    )).To((
#define EACH_BLOCK_IS         )).DividedBy((
#define CACHE                 )).SetCacheFileName((
#define INIT_RESULT           )).SetResult((

#define THREADS               )).SetThreadsCount((
#define MAP                   )).Map((        [=] (int64 key, int64 worker)-> int64
#define BLOCK_MAP             )).BlockMap((  [=] (int64 first, int64 last, int64 worker)-> int64
#define REDUCE                )).Reduce((     [=] (int64 result, int64 value)-> int64
#define REDUCE_SUM            )).SetReduceSum(true).SetMod(0).Start().Result()
#define REDUCE_SUM_MOD(mod)   )).SetReduceSum(true).SetMod(mod).Start().Result()
#define REDUCE_PROD           )).SetReduceProd(true).SetMod(0).Start().Result()
#define REDUCE_PROD_MOD(mod)  )).SetReduceProd(true).SetMod(mod).Start().Result()
#define END_PARALLEL          )).Start()
#define END_PARALLEL_WITH_RESULT      )).Start().Result()
#define PARALLEL_RESULT(x) (x.Result())

// The parallel interface that used to be inherited by sub-class.
// It uses ParallelRunner to schedule taskes.
// It inherits RangeBasedTaskGenerator, SupportCacheFileName to
// generate tasks and support cache.
template <typename Derived>
class ParallelRangeT : public RangeBasedTaskGenerator<ParallelRangeT<Derived>>,
                       public SupportCacheFileName<ParallelRangeT<Derived>> {
 public:
  using Self = ParallelRangeT<Derived>;
  using Task = typename RangeBasedTaskGenerator<Self>::Task;
  int64 Result() const { return result_; }

  void OnFinished() { cerr << "result = " << result_ << endl; }

  void HandleResult(const Task& /*task*/, int64 result) {
    if (reducer_internal_) {
      result_ = (this->*reducer_internal_)(result_, result);
    } else {
      auto& obj = static_cast<Derived&>(*this);
      result_ = obj.UpdateResult(result_, result);
    }
  }

  bool HandleCachedResult(const Task& task, int64 result) {
    HandleResult(task, result);
    return true;
  }

  int64 UpdateResult(int64 result, int64 /*value*/) { return result; }

  int64 WorkOnBlock(int64 /*first*/, int64 /*last*/, int64 /*worker*/) {
    return 0;
  }

  int64 Work(const Task& task, int64 worker_idx) {
    auto& obj = static_cast<Derived&>(*this);
    auto range = obj.TaskIdToRange(task.id);
    return obj.WorkOnBlock(range.first, range.second, worker_idx);
  }

  Self& From(int64 first) {
    this->first_ = first;
    return *this;
  }

  Self& To(int64 last) {
    this->last_ = last;
    return *this;
  }

  Self& DividedBy(int64 block_size) {
    this->block_size_ = block_size;
    return *this;
  }

  Self& SetCacheFileName(const char* file_name) {
    SupportCacheFileName<ParallelRange>::SetCacheFileName(file_name);
    return *this;
  }

  Self& SetResult(int64 result) {
    this->result_ = result;
    return *this;
  }

  Self& SetThreadsCount(int threads_count) {
    this->threads_count_ = threads_count;
    return *this;
  }

  Self& SetReduceSum(bool reduce_sum) {
    reduce_sum_ = reduce_sum;
    return *this;
  }

  Self& SetReduceProd(bool reduce_prod) {
    reduce_prod_ = reduce_prod;
    return *this;
  }

  Self& SetMod(int64 mod) {
    mod_ = mod;
    return *this;
  }

  Self& Start() {
    this->SetRange(this->first_, this->last_, this->block_size_);
    runner_.Run(*this, threads_count_);
    return *this;
  }

  void Run(int threads_count) { runner_.Run(*this, threads_count); }

 protected:
  void BindReducer() {
    if (reduce_sum_) {
      if (mod_) {
        reducer_internal_ = &Self::ReduceAddMod;
      } else {
        reducer_internal_ = &Self::ReduceAdd;
      }
      reducer_init_value_ = 0;
    } else if (reduce_prod_) {
      if (mod_) {
        reducer_internal_ = &Self::ReduceProdMod;
      } else {
        reducer_internal_ = &Self::ReduceProd;
      }
      reducer_init_value_ = 1;
    }
    if (reducer_internal_) {
      result_ = reducer_init_value_;
    }
  }

  int64 ReduceAdd(int64 result, int64 value) { return result + value; }

  int64 ReduceAddMod(int64 result, int64 value) {
    return AddMod(result, RegulateMod(value, mod_), mod_);
  }

  int64 ReduceProd(int64 result, int64 value) { return result * value; }

  int64 ReduceProdMod(int64 result, int64 value) {
    return MulModEx(result, RegulateMod(value, mod_), mod_);
  }

 protected:
  ParallelRunner<ParallelRangeT<Derived>> runner_;
  int64 result_;
  int threads_count_{8};
  bool reduce_sum_{false};
  bool reduce_prod_{false};
  int64 mod_{0};
  int64 reducer_init_value_{0};
  using ReducerType = int64 (Self::*)(int64, int64);
  ReducerType reducer_internal_{nullptr};
};

#endif  // end PE_PARALLEL_HAS_CPP11

#endif  // end PLATFORM_WIN

#endif
